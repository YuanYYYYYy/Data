{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyPFINiU8CxtIoTIUuJJaHbD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"xn9KSpu5aZbq","executionInfo":{"status":"ok","timestamp":1652747481587,"user_tz":420,"elapsed":187,"user":{"displayName":"qin yuan","userId":"06628153174109741505"}}},"outputs":[],"source":["from torch import nn\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp1 = Chomp1d(padding)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp2 = Chomp1d(padding)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n","                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n","        super(TCN, self).__init__()\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.linear = nn.Linear(num_channels[-1], output_size)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.linear.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        y1 = self.tcn(x)\n","        return self.linear(y1[:, :, -1])\n","import torch\n","import argparse\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import sys\n","import torch\n","import numpy as np\n","from torch.autograd import Variable\n","\n","\n","def data_generator(N, seq_length):\n","    \"\"\"\n","    Args:\n","        seq_length: Length of the adding problem data\n","        N: # of data in the set\n","    \"\"\"\n","    X_num = torch.rand([N, 1, seq_length])\n","    X_mask = torch.zeros([N, 1, seq_length])\n","    Y = torch.zeros([N, 1])\n","    for i in range(N):\n","        positions = np.random.choice(seq_length, size=2, replace=False)\n","        X_mask[i, 0, positions[0]] = 1\n","        X_mask[i, 0, positions[1]] = 1\n","        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n","    X = torch.cat((X_num, X_mask), dim=1)\n","    return Variable(X), Variable(Y)\n","\n","\n"]},{"cell_type":"code","source":["batch_size=32\n","cuda=False\n","dropout=0.0\n","#                     help='dropout applied to layers (default: 0.0)')\n","clip=-1.0\n","#                     help='gradient clip, -1 means no clip (default: -1)')\n","epochs=10\n","ksize=7\n","#                     help='kernel size (default: 7)')\n","levels=8\n","#                     help='# of levels (default: 8)')\n","seq_len=400\n","#                     help='sequence length (default: 400)')\n","log_interval=100\n","#                     help='report interval (default: 100')\n","lr=4e-3\n","#                     help='initial learning rate (default: 4e-3)')\n","\n","#                     help='optimizer to use (default: Adam)')\n","nhid=30\n","#                     help='number of hidden units per layer (default: 30)')\n","seed=1111\n","#                     help='random seed (default: 1111)')"],"metadata":{"id":"TCyZ60SFct8-","executionInfo":{"status":"ok","timestamp":1652748175559,"user_tz":420,"elapsed":194,"user":{"displayName":"qin yuan","userId":"06628153174109741505"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(seed)\n","\n","input_channels = 2\n","n_classes = 1\n","batch_size = batch_size\n","seq_length = seq_len\n","epochs = epochs\n","\n","\n","print(\"Producing data...\")\n","X_train, Y_train = data_generator(50000, int(seq_length))\n","X_test, Y_test = data_generator(1000, int(seq_length))\n","\n","\n","# Note: We use a very simple setting here (assuming all levels have the same # of channels.\n","channel_sizes = [nhid]*levels\n","kernel_size = ksize\n","dropout = dropout\n","model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n","\n","if cuda:\n","    model.cuda()\n","    X_train = X_train.cuda()\n","    Y_train = Y_train.cuda()\n","    X_test = X_test.cuda()\n","    Y_test = Y_test.cuda()\n","\n","lr = lr\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","\n","def train(epoch):\n","    global lr\n","    model.train()\n","    batch_idx = 1\n","    total_loss = 0\n","    for i in range(0, X_train.size(0), batch_size):\n","        if i + batch_size > X_train.size(0):\n","            x, y = X_train[i:], Y_train[i:]\n","        else:\n","            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = F.mse_loss(output, y)\n","        loss.backward()\n","        if clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        batch_idx += 1\n","        total_loss += loss.item()\n","\n","        if batch_idx % log_interval == 0:\n","            cur_loss = total_loss / log_interval\n","            processed = min(i+batch_size, X_train.size(0))\n","            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n","                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n","            total_loss = 0\n","\n","\n","def evaluate():\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(X_test)\n","        test_loss = F.mse_loss(output, Y_test)\n","        print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n","        return test_loss.item()\n","\n","\n","for ep in range(1, epochs+1):\n","    train(ep)\n","    tloss = evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNyLqHGabEZ6","executionInfo":{"status":"ok","timestamp":1652753588579,"user_tz":420,"elapsed":10338,"user":{"displayName":"qin yuan","userId":"06628153174109741505"}},"outputId":"4f5f45ec-17af-42b5-880e-a078b5571be7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Producing data...\n","Train Epoch:  1 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.324636\n","Train Epoch:  1 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.173538\n","Train Epoch:  1 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.176563\n","Train Epoch:  1 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.176772\n","Train Epoch:  1 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.172667\n","Train Epoch:  1 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.170086\n","Train Epoch:  1 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.174982\n","Train Epoch:  1 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.168434\n","Train Epoch:  1 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.166285\n","Train Epoch:  1 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.173971\n","Train Epoch:  1 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.168752\n","Train Epoch:  1 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.167044\n","Train Epoch:  1 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.166367\n","Train Epoch:  1 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.165887\n","Train Epoch:  1 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.173327\n","\n","Test set: Average loss: 0.169814\n","\n","Train Epoch:  2 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.165211\n","Train Epoch:  2 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.169035\n","Train Epoch:  2 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.169908\n","Train Epoch:  2 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.170152\n","Train Epoch:  2 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.167864\n","Train Epoch:  2 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.166092\n","Train Epoch:  2 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.166016\n","Train Epoch:  2 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.122497\n","Train Epoch:  2 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.014630\n","Train Epoch:  2 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.006657\n","Train Epoch:  2 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.004413\n","Train Epoch:  2 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.002469\n","Train Epoch:  2 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.002499\n","Train Epoch:  2 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.002015\n","Train Epoch:  2 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.001911\n","\n","Test set: Average loss: 0.002081\n","\n","Train Epoch:  3 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.001522\n","Train Epoch:  3 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.001183\n","Train Epoch:  3 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.001730\n","Train Epoch:  3 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000930\n","Train Epoch:  3 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.001045\n","Train Epoch:  3 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000768\n","Train Epoch:  3 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000827\n","Train Epoch:  3 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000733\n","Train Epoch:  3 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000639\n","Train Epoch:  3 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000427\n","Train Epoch:  3 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000403\n","Train Epoch:  3 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000314\n","Train Epoch:  3 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000694\n","Train Epoch:  3 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000514\n","Train Epoch:  3 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000333\n","\n","Test set: Average loss: 0.000230\n","\n","Train Epoch:  4 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000403\n","Train Epoch:  4 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000357\n","Train Epoch:  4 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000339\n","Train Epoch:  4 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000399\n","Train Epoch:  4 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000383\n","Train Epoch:  4 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.001062\n","Train Epoch:  4 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000336\n","Train Epoch:  4 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000326\n","Train Epoch:  4 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000300\n","Train Epoch:  4 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000442\n","Train Epoch:  4 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000752\n","Train Epoch:  4 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000691\n","Train Epoch:  4 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000263\n","Train Epoch:  4 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000232\n","Train Epoch:  4 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000211\n","\n","Test set: Average loss: 0.000380\n","\n","Train Epoch:  5 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000230\n","Train Epoch:  5 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000214\n","Train Epoch:  5 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000184\n","Train Epoch:  5 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000139\n","Train Epoch:  5 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000166\n","Train Epoch:  5 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000475\n","Train Epoch:  5 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000195\n","Train Epoch:  5 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000241\n","Train Epoch:  5 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000204\n","Train Epoch:  5 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000829\n","Train Epoch:  5 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000217\n","Train Epoch:  5 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000196\n","Train Epoch:  5 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000329\n","Train Epoch:  5 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000250\n","Train Epoch:  5 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000234\n","\n","Test set: Average loss: 0.000304\n","\n","Train Epoch:  6 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000226\n","Train Epoch:  6 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000172\n","Train Epoch:  6 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000709\n","Train Epoch:  6 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000194\n","Train Epoch:  6 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000391\n","Train Epoch:  6 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000821\n","Train Epoch:  6 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000252\n","Train Epoch:  6 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000127\n","Train Epoch:  6 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000198\n","Train Epoch:  6 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000228\n","Train Epoch:  6 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000172\n","Train Epoch:  6 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000321\n","Train Epoch:  6 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000381\n","Train Epoch:  6 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000267\n","Train Epoch:  6 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000190\n","\n","Test set: Average loss: 0.000218\n","\n","Train Epoch:  7 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000107\n","Train Epoch:  7 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000165\n","Train Epoch:  7 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000409\n","Train Epoch:  7 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000315\n","Train Epoch:  7 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000438\n","Train Epoch:  7 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000487\n","Train Epoch:  7 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000181\n","Train Epoch:  7 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000178\n","Train Epoch:  7 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000316\n","Train Epoch:  7 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000336\n","Train Epoch:  7 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000155\n","Train Epoch:  7 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000197\n","Train Epoch:  7 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000413\n","Train Epoch:  7 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000166\n","Train Epoch:  7 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000094\n","\n","Test set: Average loss: 0.000141\n","\n","Train Epoch:  8 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000199\n","Train Epoch:  8 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000199\n","Train Epoch:  8 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000257\n","Train Epoch:  8 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000108\n","Train Epoch:  8 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000223\n","Train Epoch:  8 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000488\n","Train Epoch:  8 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000458\n","Train Epoch:  8 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000133\n","Train Epoch:  8 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000123\n","Train Epoch:  8 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000217\n","Train Epoch:  8 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000571\n","Train Epoch:  8 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.001918\n","Train Epoch:  8 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000161\n","Train Epoch:  8 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000101\n","Train Epoch:  8 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000071\n","\n","Test set: Average loss: 0.000221\n","\n","Train Epoch:  9 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000189\n","Train Epoch:  9 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000142\n","Train Epoch:  9 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000133\n","Train Epoch:  9 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000147\n","Train Epoch:  9 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000132\n","Train Epoch:  9 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000156\n","Train Epoch:  9 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000106\n","Train Epoch:  9 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000106\n","Train Epoch:  9 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000113\n","Train Epoch:  9 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000346\n","Train Epoch:  9 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000512\n","Train Epoch:  9 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.001538\n","Train Epoch:  9 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000148\n","Train Epoch:  9 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000216\n","Train Epoch:  9 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000059\n","\n","Test set: Average loss: 0.000157\n","\n","Train Epoch: 10 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000114\n","Train Epoch: 10 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000059\n","Train Epoch: 10 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000077\n","Train Epoch: 10 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000126\n","Train Epoch: 10 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000153\n","Train Epoch: 10 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000140\n","Train Epoch: 10 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000294\n","Train Epoch: 10 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000112\n","Train Epoch: 10 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000200\n","Train Epoch: 10 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000202\n","Train Epoch: 10 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000375\n","Train Epoch: 10 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.001568\n","Train Epoch: 10 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000097\n","Train Epoch: 10 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000163\n","Train Epoch: 10 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000074\n","\n","Test set: Average loss: 0.000120\n","\n"]}]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M38FhXCVNXKJ","executionInfo":{"status":"ok","timestamp":1652760634481,"user_tz":420,"elapsed":199,"user":{"displayName":"qin yuan","userId":"06628153174109741505"}},"outputId":"c6684619-2ce9-4d31-faf5-0a0ba63d1e24"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50000, 2, 400])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["\"\"\" A very simple Temporal Convolutional Networks baseline\n","* TCN implementation is adapted from https://github.com/locuslab/TCN\n","* This is a very small dataset, so it's very easy to overfit. Tune carefully.\n","* Check the \"Output\" tab for training logs.\n","\"\"\"\n","from subprocess import call\n","import os\n","\n","FNULL = open(os.devnull, 'w')\n","call(\"pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.3.zip\".split(\" \"), stdout=FNULL, stderr=FNULL)\n","\n","# set SEED\n","os.environ[\"SEED\"] = \"42\"\n","\n","DEVICE = \"cpu\"\n","\n","# ======================\n","#     TCN Components\n","# ======================\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n","                                           stride=stride, padding=0, dilation=dilation))\n","        self.pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n","                                           stride=stride, padding=0, dilation=dilation))\n","        self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n","                                 self.pad, self.conv2, self.relu, self.dropout)\n","        self.downsample = nn.Conv1d(\n","            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x.unsqueeze(2)).squeeze(2)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","\n","# ======================\n","#     Dataset Utils\n","# ======================\n","from pathlib import Path\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","def get_dataset(x, y):\n","    return TensorDataset(\n","        torch.from_numpy(x).float(),\n","        torch.from_numpy(y).float()\n","    )\n","\n","\n","def get_dataloader(x: np.array, y: np.array, batch_size: int, shuffle: bool = True, num_workers: int = 0):\n","    dataset = get_dataset(x, y)\n","    return DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n","    )\n","\n","\n","def get_ndarray(embedding_values):\n","    results = []\n","    for row in embedding_values:\n","        arr = np.array(row)\n","        results.append(\n","            np.pad(arr, ((10 - arr.shape[0], 0), (0, 0)), 'constant')\n","        )\n","    # shape: (examples, emb_dim, seq_length)\n","    return np.transpose(np.stack(results), (0, 2, 1))\n","\n","\n","def read_dataset(data_dir=Path(\"data/\")):\n","    if isinstance(data_dir, str):\n","        data_dir = Path(data_dir)\n","    df_train = pd.read_json(data_dir / 'train.json')\n","    df_test = pd.read_json(data_dir / 'test.json')\n","    x_train = get_ndarray(df_train.audio_embedding)\n","    y_train = df_train.is_turkey.values\n","    x_test = get_ndarray(df_test.audio_embedding)\n","    test_id = df_test.vid_id\n","    return x_train, y_train, x_test, test_id\n","\n","\n","# ===============================================\n","#     Model Creation, Training, and Inference\n","# ==============================================\n","import logging\n","\n","from helperbot.bot import BaseBot\n","from helperbot.lr_scheduler import TriangularLR\n","from helperbot.weight_decay import WeightDecayOptimizerWrapper\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import log_loss, roc_auc_score\n","import torch.nn as nn\n","import pandas as pd\n","\n","class TurkeyBot(BaseBot):\n","    name = \"Turkey\"\n","\n","    def __init__(self, model, train_loader, val_loader, *, optimizer,\n","                 avg_window=20, log_dir=\"./cache/logs/\",\n","                 log_level=logging.INFO, checkpoint_dir=\"./cache/model_cache/\"):\n","        super().__init__(\n","            model, train_loader, val_loader,\n","            optimizer=optimizer, avg_window=avg_window,\n","            log_dir=log_dir, log_level=log_level, checkpoint_dir=checkpoint_dir,\n","            batch_idx=0, echo=False, device=DEVICE\n","        )\n","        self.criterion = torch.nn.BCEWithLogitsLoss()\n","        self.loss_format = \"%.8f\"\n","\n","\n","class TCNModel(nn.Module):\n","    def __init__(self, num_channels, kernel_size=2, dropout=0.2):\n","        super(TCNModel, self).__init__()\n","        self.tcn = TemporalConvNet(\n","            128, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        self.decoder = nn.Linear(num_channels[-1], 1)\n","\n","    def forward(self, x):\n","        return self.decoder(self.dropout(self.tcn(x)[:, :, -1]))\n","        \n","        \n","def main():\n","    x_train, y_train, x_test, test_id = read_dataset(\"../input/\")\n","\n","    test_loader = get_dataloader(\n","        x_test, np.zeros(x_test.shape[0]), batch_size=128, shuffle=False)\n","\n","    test_pred_list, val_losses = [], []\n","    kf = StratifiedKFold(n_splits=8, random_state=31829, shuffle=True)\n","    for train_index, valid_index in kf.split(x_train, y_train):\n","        train_loader = get_dataloader(\n","            x_train[train_index], y_train[train_index],\n","            batch_size=32, shuffle=True\n","        )\n","        val_loader = get_dataloader(\n","            x_train[valid_index], y_train[valid_index],\n","            batch_size=128, shuffle=False\n","        )\n","\n","        model = TCNModel(num_channels=[20] * 2, kernel_size=3, dropout=0.25)\n","        model.to(DEVICE)\n","        optimizer = torch.optim.Adam(\n","            model.parameters(), betas=(0.9, 0.999), lr=1e-3, weight_decay=0)\n","        # optimizer = WeightDecayOptimizerWrapper(\n","        #     optimizer, weight_decay=5e-3\n","        # )\n","        batches_per_epoch = len(train_loader)\n","        bot = TurkeyBot(\n","            model, train_loader, val_loader,\n","            optimizer=optimizer, avg_window=batches_per_epoch\n","        )\n","        n_steps = batches_per_epoch * 20\n","        scheduler = TriangularLR(\n","            optimizer, max_mul=8, ratio=9,\n","            steps_per_cycle=n_steps\n","        )\n","        bot.train(\n","            n_steps,\n","            log_interval=batches_per_epoch // 2,\n","            snapshot_interval=batches_per_epoch,\n","            early_stopping_cnt=10, scheduler=scheduler)\n","        val_preds = torch.sigmoid(bot.predict_avg(\n","            val_loader, k=3, is_test=True).cpu()).numpy().clip(1e-5, 1-1e-5)\n","        loss = log_loss(y_train[valid_index], val_preds)\n","        if loss > 0.2:\n","            # Ditch folds that perform terribly\n","            bot.remove_checkpoints(keep=0)\n","            continue\n","        print(\"AUC: %.6f\" % roc_auc_score(y_train[valid_index], val_preds))\n","        print(\"Val loss: %.6f\" % loss)\n","        val_losses.append(loss)\n","        test_pred_list.append(torch.sigmoid(bot.predict_avg(\n","            test_loader, k=3, is_test=True).cpu()).numpy().clip(1e-5, 1-1e-5))\n","        bot.remove_checkpoints(keep=0)\n","\n","    val_loss = np.mean(val_losses)\n","    test_preds = np.mean(test_pred_list, axis=0)\n","    print(\"Validation losses: %.6f +- %.6f\" %\n","          (np.mean(val_losses), np.std(val_losses)))\n","\n","    df_sub = pd.DataFrame({\n","        \"vid_id\": test_id,\n","        \"is_turkey\": test_preds\n","    })\n","    df_sub.to_csv(\"submission.csv\", index=False)\n","\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"Cvaelpg-bWd4","executionInfo":{"status":"error","timestamp":1652747681165,"user_tz":420,"elapsed":13690,"user":{"displayName":"qin yuan","userId":"06628153174109741505"}},"outputId":"45875830-5428-4a1d-d03b-83589f02b615"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-61fe80dac4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mdf_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-61fe80dac4ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     test_loader = get_dataloader(\n","\u001b[0;32m<ipython-input-6-61fe80dac4ea>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'test.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected object or value"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"D61hdhBzcAap"},"execution_count":null,"outputs":[]}]}